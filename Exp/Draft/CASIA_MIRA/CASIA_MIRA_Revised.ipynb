{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd09afce5eaa262f9eb73f116525a554221be3c912216eaeaff025e9a27aced35f2",
   "display_name": "Python 3.8.8 64-bit ('tfenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "source": [
    "一个用于生成某一层节点的函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_blocl(filter_arr,kernel_size_arr,stride_arr,inputs,times):\n",
    "\n",
    "    x=inputs;\n",
    "    for i in range(0,times):\n",
    "        x=tf.keras.layers.Conv2D(filters=filter_arr[0],kernel_size=(kernel_size_arr[0],kernel_size_arr[0]),strides=(stride_arr[0],stride_arr[0]),kernel_initializer='he_normal',padding='same')(x)\n",
    "        x=tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "        x=tf.keras.layers.Activation('elu')(x)\n",
    "\n",
    "        x=tf.keras.layers.Conv2D(filters=filter_arr[1],kernel_size=(kernel_size_arr[1],kernel_size_arr[1]),strides=(stride_arr[1],stride_arr[1]),kernel_initializer='he_normal',padding='same')(x)\n",
    "        x=tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "        x=tf.keras.layers.Activation('elu')(x)\n",
    "    \n",
    "        output=tf.keras.layers.add([inputs,x])\n",
    "        output=tf.nn.relu(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_tripple_block(filter_arr,kernel_size_arr,stride_arr,inputs,times):\n",
    "    x=inputs;\n",
    "    for i in range(0,times):\n",
    "        x=tf.keras.layers.Conv2D(filters=filter_arr[0],kernel_size=(kernel_size_arr[0],kernel_size_arr[0]),strides=(stride_arr[0],stride_arr[0]),kernel_initializer='he_normal',padding='same')(x)\n",
    "        x=tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "        x=tf.keras.layers.Activation('elu')(x)\n",
    "\n",
    "        x=tf.keras.layers.Conv2D(filters=filter_arr[0],kernel_size=(kernel_size_arr[1],kernel_size_arr[1]),strides=(stride_arr[1],stride_arr[1]),kernel_initializer='he_normal',padding='same')(x)\n",
    "        x=tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "        x=tf.keras.layers.Activation('elu')(x)\n",
    "\n",
    "        x=tf.keras.layers.Conv2D(filters=filter_arr[1],kernel_size=(kernel_size_arr[2],kernel_size_arr[2]),strides=(stride_arr[2],stride_arr[2]),kernel_initializer='he_normal',padding='same')(x)\n",
    "        x=tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "        x=tf.keras.layers.Activation('elu')(x)\n",
    "    \n",
    "        output=tf.keras.layers.add([inputs,x])\n",
    "        output=tf.nn.relu(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/gdrive/MyDrive/ISBI_Dataset\")\n",
    "X_ids = next(os.walk('train'))[2]\n",
    "Y_ids = next(os.walk('label'))[2]\n",
    "print(len(X_ids),len(Y_ids))\n",
    "X_ids.sort()\n",
    "Y_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS, IMG_WIDTH, IMG_HEIGHT = 3, 512, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_symmetry(img_input):\n",
    "    return np.array(img_input)[::-1,:,:]\n",
    "    \n",
    "def horizental_symmetry(img_input):\n",
    "    return np.array(img_input)[:,::-1,:]\n",
    "\n",
    "def vertical_symmetry_2D(img_input):\n",
    "    return np.array(img_input)[::-1,:]\n",
    "    \n",
    "def horizental_symmetry_2D(img_input):\n",
    "    return np.array(img_input)[:,::-1]\n",
    "    \n",
    "def cropping(img_input):\n",
    "    return img_input.crop((0,0,img_input.width/2,img_input.height/2)),img_input.crop((0,img_input.height/2-1,img_input.width/2,img_input.height-1)),img_input.crop((img_input.width/2-1,0,img_input.width-1,img_input.height/2-1)),img_input.crop((img_input.width/2-1,img_input.height/2-1,img_input.width-1,img_input.height-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(X_ids)*7, 256, 256, 3), dtype=np.float32)\n",
    "Y_train = np.zeros((len(Y_ids)*7, 256, 256, 1), dtype=np.bool)\n",
    "X_original=np.zeros((len(X_ids),512,512,3),dtype=np.float32)\n",
    "Y_original=np.zeros((len(Y_ids),512,512,1),dtype=np.float32)\n",
    "\n",
    "n=0\n",
    "m=0\n",
    "for id_ in (X_ids):\n",
    "    image = tf.keras.preprocessing.image.load_img(f'/gdrive/MyDrive/ISBI_Dataset/train/{id_}', target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    # print(n,id_)\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)[90:450,150:406]\n",
    "    image = tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((256, 256))\n",
    "    original_image=tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((512, 512))\n",
    "    X_train[n] = np.array(image)\n",
    "    X_original[m]=np.array(original_image)\n",
    "    img1,img2,img3,img4=cropping(tf.keras.preprocessing.image.array_to_img(X_original[m]))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(img1.resize((256, 256)))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(img2.resize((256, 256)))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(img3.resize((256, 256)))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(img4.resize((256, 256)))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(tf.keras.preprocessing.image.array_to_img(vertical_symmetry(tf.keras.preprocessing.image.array_to_img(X_train[])),).resize((256, 256)))\n",
    "    n+=1\n",
    "    X_train[n]=np.array(tf.keras.preprocessing.image.array_to_img(horizental_symmetry(tf.keras.preprocessing.image.array_to_img(X_train[temp])),).resize((256, 256)))\n",
    "    n+=1\n",
    "    m+=1\n",
    "\n",
    "n=0\n",
    "m=0\n",
    "for  id_ in (Y_ids):\n",
    "    image = tf.keras.preprocessing.image.load_img(f'/gdrive/MyDrive/ISBI_Dataset/label/{id_}', \n",
    "                                                  target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=\"grayscale\")\n",
    "\n",
    "    print(n,id_)\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)[90:450,150:406]\n",
    "    image = tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((256, 256))\n",
    "    image = tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((256, 256))\n",
    "    original_image=tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((512, 512))\n",
    "    Y_train[n] = np.array(image)[:, :, np.newaxis]\n",
    "    Y_original[m]=np.array(original_image)[:, :, np.newaxis]\n",
    "    \n",
    "    img1,img2,img3,img4=cropping(tf.keras.preprocessing.image.array_to_img(Y_original[m]))\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(img1.resize((256, 256)))[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(img2.resize((256, 256)))[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(img3.resize((256, 256)))[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(img4.resize((256, 256)))[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(vertical_symmetry_2D(tf.keras.preprocessing.image.array_to_img(Y_train[temp])),)[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    Y_train[n]=np.array(horizental_symmetry_2D(tf.keras.preprocessing.image.array_to_img(Y_train[temp])),)[:, :, np.newaxis]\n",
    "    n+=1\n",
    "    m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(121)\n",
    "ax1.imshow(tf.keras.preprocessing.image.array_to_img(X_train[29]))\n",
    "ax2=plt.subplot(122)\n",
    "ax2.imshow(tf.keras.preprocessing.image.array_to_img(Y_train[29]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tf.keras.layers.Input((512,512,3))\n",
    "c0=tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),activation='elu',kernel_initializer='he_normal',padding='same')(inputs)\n",
    "p0=tf.keras.layers.MaxPool2D(pool_size=(3,3),strides=(2,2))(c0)\n",
    "Deconv2=tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=(4,4),strides=(2,2),kernel_initializer='he_normal')(p0)\n",
    "\n",
    "block1=build_single_block(filter_arr=[128,128],kernel_size_arr=[3,3],stride_arr=[2,1],inputs=p0,times=1)\n",
    "block1=build_single_block(filter_arr=[128,128],kernel_size_arr=[3,3],stride_arr=[1,1],inputs=block1,times=2)\n",
    "Deconv3=tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=(8,8),strides=(4,4),kernel_initializer='he_normal',padding='same')(block1)\n",
    "\n",
    "block2=build_single_block(filter_arr=[256,256],kernel_size_arr=[3,3],stride_arr=[2,1],inputs=block1,times=1)\n",
    "block2=build_single_block(filter_arr=[256,256],kernel_size_arr=[3,3],stride_arr=[1,1],inputs=block2,times=1)\n",
    "\n",
    "block3=build_single_block(filter_arr=[512,512],kernel_size_arr=[3,3],stride_arr=[1,1],inputs=block2,times=6)\n",
    "\n",
    "block4=build_single_block(filter_arr=[512,1024],kernel_size_arr=[3,3],stride_arr=[1,1],inputs=block3,times=3)\n",
    "\n",
    "# block5=build_single_block(filter_arr=[512,1024],kernel_size_arr=[1,3],stride_arr=[1,1],inputs=block4,times=1)\n",
    "# block6=build_single_block(filter_arr=[2048,1024],kernel_size_arr=[1,1],stride_arr=[1,1],inputs=block5,times=1)\n",
    "# block6=build_single_block(filter_arr=[2048,4096],kernel_size_arr=[3,1],stride_arr=[1,1],inputs=block6,times=1)\n",
    "\n",
    "block5=build_tripple_block(filter_arr=[512,1024,2048],kernel_size_arr=[1,3,1],stride_arr=[1,1,1],inputs=block4,times=1)\n",
    "block6=build_tripple_block(filter_arr=[1024,2048,4096],kernel_size_arr=[1,3,1],stride_arr=[1,1,1],inputs=block5,times=1)\n",
    "\n",
    "\n",
    "Deconv1=tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=(16,16),strides=(8,8),padding='same',kernel_initializer='he_normal')(block6)\n",
    "\n",
    "fusion=tf.add(Deconv1,Deconv2)\n",
    "fusion=tf.add(fusion,Deconv3)\n",
    "\n",
    "outConv1=tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),kernel_initializer='he_normal',padding='same')(fusion)\n",
    "d1=tf.keras.layers.Dropout(rate=0.5)(outConv1)\n",
    "outConv2=tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),strides=(1,1),kernel_initializer='he_normal',padding='same')(d1)\n",
    "outputs=tf.keras.layers.Softmax()(outConv2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=8, epochs=1000)\n",
    "model.save(\"CASIA_MIRA.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_id = random.choice(X_ids)\n",
    "print(test_id)\n",
    "img = tf.keras.preprocessing.image.load_img(f\"/gdrive/MyDrive/ISBI_Dataset/train/{test_id}\", target_size=(256, 256))\n",
    "input_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "input_array_model = np.array([input_array])\n",
    "predictions = model.predict(input_array_model)\n",
    "ax10=plt.subplot(121)\n",
    "ax10.imshow(np.asarray(Image.open(f\"/gdrive/MyDrive/ISBI_Dataset/train/{test_id}\")))\n",
    "ax11=plt.subplot(122)\n",
    "ax11.imshow(tf.keras.preprocessing.image.array_to_img(np.squeeze(predictions)[:, :, np.newaxis]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax12=plt.subplot(121)\n",
    "ax12.imshow(np.asarray(tf.keras.preprocessing.image.array_to_img(np.squeeze(predictions)[:, :, np.newaxis])))\n",
    "ax13=plt.subplot(122)\n",
    "train_id=test_id.replace('volume','labels')\n",
    "ax13.imshow(np.asarray(Image.open(f\"/gdrive/MyDrive/ISBI_Dataset/label/{train_id}\")))\n",
    "plt.show()"
   ]
  }
 ]
}